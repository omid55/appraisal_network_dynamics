{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# Full pipeline for the influence matrix estimation problem on the supervised dataset from the Jeopardy-like logs for comparison on different models\n",
    "# ===========================================================\n",
    "\n",
    "Goals:\n",
    "1. Split the data into test and train, and validation for multiple runs\n",
    "2. Formulate all different models of convex optimization, neural networks, and tower models.\n",
    "3. Give the same splits to all models, tune the hyperparameters with validation set, and report train and test erros as a pickle, a table, and a figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Started on: 30 Dec 2019\n",
    "#### Last update: 02 Jan 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import, unicode_literals\n",
    "\n",
    "import imp\n",
    "import sys\n",
    "import scipy as sp\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, LSTM, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Concatenate, Reshape, Embedding, Dot\n",
    "from tensorflow.keras.models import Model\n",
    "sys.path.insert(0, '../src/')\n",
    "%matplotlib inline\n",
    "\n",
    "import utils\n",
    "import mytools\n",
    "from mytools import Timer\n",
    "from mytools import Tee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = '/home/omid/Datasets/Jeopardy/supervised_data.pk'\n",
    "# DATA_FILE_PATH = '/home/omid/Datasets/Jeopardy/supervised_data_roberta.pk'\n",
    "TEST_FRACTION = 0.2\n",
    "RUNS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload():\n",
    "    imp.reload(utils)\n",
    "    imp.reload(mytools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_eigvec_of_laplacian(A: np.matrix) -> np.matrix:\n",
    "# #     D = np.diag(np.array(np.sum(A, axis=0))[0])\n",
    "# #     L = D - A\n",
    "# #     return np.matrix(np.linalg.eig(L)[1])\n",
    "#     n, m = A.shape\n",
    "#     diags = A.sum(axis=1).flatten()\n",
    "#     D = sp.sparse.spdiags(diags, [0], m, n, format='csr')\n",
    "#     L = D - A\n",
    "#     with sp.errstate(divide='ignore'):\n",
    "#         diags_sqrt = 1.0/sp.sqrt(diags)\n",
    "#     diags_sqrt[sp.isinf(diags_sqrt)] = 0\n",
    "#     DH = sp.sparse.spdiags(diags_sqrt, [0], m, n, format='csr')\n",
    "#     DH = DH.todense()\n",
    "#     normalized_L = DH.dot(L.dot(DH))\n",
    "#     return normalized_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comput_error(y_train_or_validation_or_test_true, y_train_or_validation_or_test_predicted, estimation_name, error_type_str):\n",
    "    \"\"\"Computes the error.\"\"\"\n",
    "    err = 0\n",
    "    for index in range(len(y_train_or_validation_or_test_true)):\n",
    "        groundtruth = y_train_or_validation_or_test_true[index][estimation_name]\n",
    "        predicted = y_train_or_validation_or_test_predicted[index]\n",
    "        # TODO(@Omid): CHECK WHETHER THIS IS JUST AN INT THEN FIND THE MOST INFLUENTIAL PERSON FROM THE ESTIAMTED MATRIX.\n",
    "        err += utils.matrix_estimation_error(\n",
    "            true_matrix=groundtruth, pred_matrix=predicted, type_str=error_type_str)\n",
    "    err /= len(y_train_or_validation_or_test_true)\n",
    "    return err\n",
    "\n",
    "\n",
    "def model_builder(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        feature_names,\n",
    "        estimation_name='influence_matrix',\n",
    "        error_type_str='normalized_frob_norm',\n",
    "        tune_hyperparameters_by_validation=True,\n",
    "        with_replication=True,\n",
    "        lambdas = [0, 0.1, 1, 10, 100, 1000],\n",
    "        model_func='average',\n",
    "        params={'with_constraints': True, 'n_splits': 3}):\n",
    "    \n",
    "    # For the baseline models.\n",
    "    if model_func == 'average':\n",
    "        mats = []\n",
    "        for i in range(len(y_train)):\n",
    "            mats.append(y_train[i][estimation_name])\n",
    "        y_baseline_predicted = [np.matrix(np.mean(mats, axis=0)) for _ in range(len(y_train))]\n",
    "    elif model_func == 'uniform':\n",
    "        y_baseline_predicted = [np.matrix(np.ones((4, 4)) * 0.25) for _ in range(len(y_train))]\n",
    "    elif model_func == 'random':\n",
    "        y_baseline_predicted = [np.matrix(\n",
    "            utils.make_matrix_row_stochastic(\n",
    "                np.random.rand(4, 4))) for _ in range(len(y_train))]\n",
    "    if model_func in ['average', 'uniform', 'random']:\n",
    "        y_train_average = []\n",
    "        train_error = comput_error(\n",
    "            y_train, y_baseline_predicted, estimation_name=estimation_name, error_type_str=error_type_str)\n",
    "        test_error = comput_error(\n",
    "            y_test, y_baseline_predicted, estimation_name=estimation_name, error_type_str=error_type_str)\n",
    "        return train_error, test_error, _\n",
    "    \n",
    "    # For the proposed models.\n",
    "    validation_errors = defaultdict(lambda: 0)\n",
    "    if tune_hyperparameters_by_validation:\n",
    "        print('{}-fold validation ...'.format(params['n_splits']))\n",
    "        kf = KFold(n_splits=params['n_splits'])\n",
    "        for train_index, validation_index in kf.split(X_train):\n",
    "            X_train_subset, X_validation = X_train[train_index], X_train[validation_index]\n",
    "            y_train_subset, y_validation = y_train[train_index], y_train[validation_index]\n",
    "            if with_replication:\n",
    "                X_train_subset, y_train_subset = utils.replicate_matrices_in_train_dataset_with_reordering(\n",
    "                    X_train_subset, y_train_subset)\n",
    "                X_train_subset = np.array(X_train_subset)\n",
    "                y_train_subset = np.array(y_train_subset)\n",
    "            print('Shapes of train: {}, validation: {}, test: {}.'.format(\n",
    "                X_train_subset.shape, X_validation.shape, X_test.shape))\n",
    "            for lambdaa in lambdas:\n",
    "                validation_errors[lambdaa] += model_func(\n",
    "                    X_train=X_train_subset,\n",
    "                    y_train=y_train_subset,\n",
    "                    X_validation_or_test=X_validation,\n",
    "                    y_validation_or_test=y_validation,\n",
    "                    feature_names=feature_names,\n",
    "                    estimation_name=estimation_name,\n",
    "                    lambdaa=lambdaa,\n",
    "                    error_type_str=error_type_str,\n",
    "                    params=params)[1]\n",
    "        best_lambda = min(validation_errors, key=validation_errors.get)\n",
    "    else:\n",
    "        best_lambda = 0.1\n",
    "    print('Training with the best lambda: {} on entire training set...'.format(best_lambda))\n",
    "    train_error, test_error = model_func(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_validation_or_test=X_test,\n",
    "        y_validation_or_test=y_test,\n",
    "        feature_names=feature_names,\n",
    "        estimation_name=estimation_name,\n",
    "        lambdaa=best_lambda,\n",
    "        error_type_str=error_type_str,\n",
    "        params=params)\n",
    "    return train_error, test_error, validation_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convex_optimization_model_func(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation_or_test,\n",
    "        y_validation_or_test,\n",
    "        feature_names,\n",
    "        estimation_name,\n",
    "        lambdaa,\n",
    "        error_type_str,\n",
    "        params={'with_constraints': True}):\n",
    "    \n",
    "    def predict(data_element, feature_names, B, Ws, is_solved=False):\n",
    "        \"\"\"Defines the prediction function.\"\"\"\n",
    "        predicted = 0\n",
    "        if is_solved:\n",
    "            predicted += B.value\n",
    "        else:\n",
    "            predicted += B\n",
    "        for feature_name in feature_names:\n",
    "            if is_solved:\n",
    "                W_for_this_feature = Ws[feature_name].value\n",
    "            else:\n",
    "                W_for_this_feature = Ws[feature_name]\n",
    "            if len(data_element[feature_name].shape) == 1:\n",
    "                # If it was a vector, it makes a matrix with it.\n",
    "                p = data_element[feature_name]\n",
    "                data_element_matrix = np.row_stack([p, p, p, p])\n",
    "            else:\n",
    "                # Unless it is already a matrix.\n",
    "                data_element_matrix = data_element[feature_name]\n",
    "            predicted += (data_element_matrix * W_for_this_feature)\n",
    "        return predicted\n",
    "    \n",
    "    def predict_all_after_solving(X_train_or_validation_or_test, B, Ws, feature_names):\n",
    "        \"\"\"Predicts for all data points in the given set.\"\"\"\n",
    "        return [\n",
    "            predict(\n",
    "                data_element=data_element,\n",
    "                feature_names=feature_names,\n",
    "                B=B,\n",
    "                Ws=Ws,\n",
    "                is_solved=True) \n",
    "            for data_element in X_train_or_validation_or_test]\n",
    "\n",
    "    # Creating variables.\n",
    "    Ws = {}\n",
    "    for feature_name in feature_names:\n",
    "        if len(X_train[0][feature_name].shape) == 1:\n",
    "            # If it was a vector, it makes a matrix with it.\n",
    "            Ws[feature_name] = cp.Variable(\n",
    "                len(X_train[0][feature_name]), len(X_train[0][feature_name]))\n",
    "        else:\n",
    "            # Unless it is already a matrix.\n",
    "            Ws[feature_name] = cp.Variable(\n",
    "                X_train[0][feature_name].shape[1], X_train[0][feature_name].shape[0])\n",
    "    B = cp.Variable(4, 4)\n",
    "\n",
    "    # Computing loss.\n",
    "    constraints = []\n",
    "    losses = 0\n",
    "    for index in range(len(X_train)):\n",
    "        element = X_train[index]\n",
    "        estimation_groundtruth = y_train[index][estimation_name]\n",
    "\n",
    "        # Defining the estimation function.\n",
    "        estimation_predicted = predict(\n",
    "            data_element=element, feature_names=feature_names, B=B, Ws=Ws, is_solved=False)\n",
    "\n",
    "        # Defining the loss function.\n",
    "        loss = cp.sum_squares(estimation_predicted - estimation_groundtruth)\n",
    "\n",
    "        losses += loss\n",
    "        if params['with_constraints']:\n",
    "            constraints += [estimation_predicted >= 0]\n",
    "            constraints += [cp.sum_entries(estimation_predicted, axis=1) == 1]\n",
    "\n",
    "    # Computing regularization.\n",
    "    regluarization = cp.norm1(B)\n",
    "    for feature_name in feature_names:\n",
    "        regluarization += cp.norm1(Ws[feature_name])\n",
    "\n",
    "    # Solving the convex problem.\n",
    "    objective = cp.Minimize(losses + lambdaa * regluarization)\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    result = prob.solve(solver=cp.MOSEK)\n",
    "    print('The status of solution was: {} and the result was: {}'.format(prob.status, result))\n",
    "\n",
    "    # Predicting and computing trian error.\n",
    "    y_train_predicted = predict_all_after_solving(\n",
    "        X_train_or_validation_or_test=X_train, B=B, Ws=Ws, feature_names=feature_names)\n",
    "    train_error = comput_error(\n",
    "        y_train_or_validation_or_test_true=y_train,\n",
    "        y_train_or_validation_or_test_predicted=y_train_predicted,\n",
    "        estimation_name=estimation_name,\n",
    "        error_type_str=error_type_str)\n",
    "    \n",
    "    # Predicting and computing validation or test error.\n",
    "    y_validation_or_test_predicted = predict_all_after_solving(\n",
    "        X_train_or_validation_or_test=X_validation_or_test, B=B, Ws=Ws, feature_names=feature_names)\n",
    "    validation_or_test_error = comput_error(\n",
    "        y_train_or_validation_or_test_true=y_validation_or_test,\n",
    "        y_train_or_validation_or_test_predicted=y_validation_or_test_predicted,\n",
    "        estimation_name=estimation_name,\n",
    "        error_type_str=error_type_str)\n",
    "    \n",
    "    return train_error, validation_or_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatinated_deep_neural_network_model_func(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation_or_test,\n",
    "        y_validation_or_test,\n",
    "        feature_names,\n",
    "        estimation_name,\n",
    "        lambdaa,\n",
    "        error_type_str,\n",
    "        params={'n_epochs': 10, 'batch_size': 32}):\n",
    "    \n",
    "#     def predict(data_element, feature_names, B, Ws, is_solved=False):\n",
    "#         \"\"\"Defines the prediction function.\"\"\"\n",
    "#         return 0\n",
    "    \n",
    "#     def predict_all_after_solving(X_train_or_validation_or_test, B, Ws, feature_names):\n",
    "#         \"\"\"Predicts for all data points in the given set.\"\"\"\n",
    "#         return []\n",
    "\n",
    "    flatten_X_train = []\n",
    "    flatten_y_train = []\n",
    "    for i in range(len(X_train)):\n",
    "        features = X_train[i]\n",
    "        label = y_train[i][estimation_name]            \n",
    "        flatten_X_train.append(np.hstack(\n",
    "            [np.array(features[feature_name].flatten())[0] for feature_name in feature_names]))\n",
    "        flatten_y_train.append(np.array(label.flatten())[0])\n",
    "    flatten_X_train = np.array(flatten_X_train)\n",
    "    flatten_y_train = np.array(flatten_y_train)\n",
    "\n",
    "    flatten_X_validation_or_test = []\n",
    "    flatten_y_validation_or_test = []\n",
    "    for i in range(len(X_validation_or_test)):\n",
    "        features = X_validation_or_test[i]\n",
    "        label = y_validation_or_test[i][estimation_name]\n",
    "        flatten_X_validation_or_test.append(np.hstack(\n",
    "            [np.array(features[feature_name].flatten())[0] for feature_name in feature_names]))\n",
    "        flatten_y_validation_or_test.append(np.array(label.flatten())[0])\n",
    "    flatten_X_validation_or_test = np.array(flatten_X_validation_or_test)\n",
    "    flatten_y_validation_or_test = np.array(flatten_y_validation_or_test)\n",
    "                              \n",
    "    _, input_size = flatten_X_train.shape\n",
    "    print('Input size for the neural network was: {}'.format(input_size))\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(\n",
    "            units=32,\n",
    "            kernel_initializer='he_normal',\n",
    "            activation='relu',\n",
    "            input_shape=(input_size,),\n",
    "            kernel_regularizer=regularizers.l1(lambdaa),\n",
    "            activity_regularizer=regularizers.l1(lambdaa)),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(\n",
    "#             units=64,\n",
    "#             kernel_initializer='he_normal',\n",
    "#             activation='relu',\n",
    "#             kernel_regularizer=regularizers.l1(lambdaa),\n",
    "#             activity_regularizer=regularizers.l1(lambdaa)),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(\n",
    "#             units=32,\n",
    "#             kernel_initializer='he_normal',\n",
    "#             activation='relu',\n",
    "#             kernel_regularizer=regularizers.l1(lambdaa),\n",
    "#             activity_regularizer=regularizers.l1(lambdaa)),\n",
    "#         Dropout(0.5),\n",
    "        Dense(16, kernel_initializer=my_init, activation='softmax')])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(flatten_X_train, flatten_y_train, epochs=params['n_epochs'], batch_size=params['batch_size'])\n",
    "\n",
    "    # Predicting and computing trian error.\n",
    "    y_train_predicted = [utils.make_matrix_row_stochastic(\n",
    "        np.matrix(np.reshape(element, (4, 4)))) for element in model.predict(flatten_X_train)]\n",
    "    train_error = comput_error(\n",
    "        y_train_or_validation_or_test_true=y_train,\n",
    "        y_train_or_validation_or_test_predicted=y_train_predicted,\n",
    "        estimation_name=estimation_name,\n",
    "        error_type_str=error_type_str)\n",
    "                                            \n",
    "    # Predicting and computing trian error.\n",
    "    y_test_predicted = [utils.make_matrix_row_stochastic(\n",
    "        np.matrix(np.reshape(element, (4, 4)))) for element in model.predict(flatten_X_validation_or_test)]\n",
    "    validation_or_test_error = comput_error(\n",
    "        y_train_or_validation_or_test_true=y_test,\n",
    "        y_train_or_validation_or_test_predicted=y_test_predicted,\n",
    "        estimation_name=estimation_name,\n",
    "        error_type_str=error_type_str)\n",
    "\n",
    "    return train_error, validation_or_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "data = utils.load_it(DATA_FILE_PATH)\n",
    "print(len(data['X']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just to see how much headroom there is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25784649, 0.25013211, 0.25917224, 0.23284916],\n",
       "       [0.21997335, 0.3244428 , 0.22187468, 0.23370918],\n",
       "       [0.21050033, 0.25039752, 0.31083892, 0.22826324],\n",
       "       [0.25180346, 0.23839655, 0.24293584, 0.26686416]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mats = []\n",
    "for i in range(len(data['y'])):\n",
    "    mats.append(data['y'][i]['influence_matrix'])\n",
    "np.mean(mats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25784649, 0.25013211, 0.25917224, 0.23284916],\n",
       "       [0.21997335, 0.3244428 , 0.22187468, 0.23370918],\n",
       "       [0.21050033, 0.25039752, 0.31083892, 0.22826324],\n",
       "       [0.25180346, 0.23839655, 0.24293584, 0.26686416]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11677549, 0.08129962, 0.08962429, 0.07536874],\n",
       "       [0.08929333, 0.16587926, 0.08335618, 0.10916787],\n",
       "       [0.08963767, 0.12213777, 0.16865608, 0.09193362],\n",
       "       [0.09597464, 0.06509444, 0.05915974, 0.08748726]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(mats, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message: It seems the std is small, and average is close to 0.25 everywhere but main diagonal which is slightly larger (due to having selfish people)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['individual_performance', 'sentiment', 'reply_duration', 'emotion_arousal', 'content_embedding_matrix', 'emotion_dominance', 'first_influence_matrix', 'individual_performance_hardness_weighted', 'emotion_valence', 'average_of_previous_influence_matrices'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['X'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDAS = [0, 0.1, 1, 10, 100, 1000]\n",
    "WITH_REPLICATION = True\n",
    "ERROR_TYPE_STRS = ['normalized_frob_norm', 'mse', 'neg_corr', 'cosine_dist']\n",
    "TUNE_HYPERPARAMETERS_BY_VALIDATION = True\n",
    "SELECTED_MODEL_FUNCS = ['average', 'uniform', 'random', convex_optimization_model_func, concatinated_deep_neural_network_model_func]\n",
    "FEATURE_NAMES_SET = [['individual_performance'], ['first_influence_matrix'], ['reply_duration'], ['emotion_dominance'], ['content_embedding_matrix'], ['average_of_previous_influence_matrices']]\n",
    "ESTIMATION_NAME = 'influence_matrix' #'most_influentials'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation comparison pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 =>>>\n",
      "\tFeatures:  ['individual_performance'] ...\n",
      "\t\tModel:  average ...\n",
      "\t\t\tError type:  normalized_frob_norm\n",
      "It took 0.03 seconds.\n",
      "\t\t\tError type:  mse\n",
      "It took 0.03 seconds.\n",
      "\t\t\tError type:  neg_corr\n",
      "It took 0.05 seconds.\n",
      "\t\t\tError type:  cosine_dist\n",
      "It took 0.06 seconds.\n",
      "\t\tModel:  uniform ...\n",
      "\t\t\tError type:  normalized_frob_norm\n",
      "It took 0.01 seconds.\n",
      "\t\t\tError type:  mse\n",
      "It took 0.01 seconds.\n",
      "\t\t\tError type:  neg_corr\n",
      "It took 0.02 seconds.\n",
      "\t\t\tError type:  cosine_dist\n",
      "It took 0.02 seconds.\n",
      "\t\tModel:  random ...\n",
      "\t\t\tError type:  normalized_frob_norm\n",
      "It took 0.02 seconds.\n",
      "\t\t\tError type:  mse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.local/lib/python3.5/site-packages/scipy/stats/stats.py:3038: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.02 seconds.\n",
      "\t\t\tError type:  neg_corr\n",
      "It took 0.03 seconds.\n",
      "\t\t\tError type:  cosine_dist\n",
      "It took 0.03 seconds.\n",
      "\t\tModel:  convex_optimization_model_func ...\n",
      "\t\t\tError type:  normalized_frob_norm\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 527.695472426736\n",
      "The status of solution was: optimal and the result was: 528.1679689566333\n",
      "The status of solution was: optimal and the result was: 532.4087398668854\n",
      "The status of solution was: optimal and the result was: 573.6432818175773\n",
      "The status of solution was: optimal and the result was: 937.7197627373182\n",
      "The status of solution was: optimal and the result was: 4537.719759395752\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 582.6098922009489\n",
      "The status of solution was: optimal and the result was: 583.0769548464941\n",
      "The status of solution was: optimal and the result was: 587.2692827405871\n",
      "The status of solution was: optimal and the result was: 628.0387376918154\n",
      "The status of solution was: optimal and the result was: 991.3514789334777\n",
      "The status of solution was: optimal and the result was: 4591.351435725568\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 615.3088193466681\n",
      "The status of solution was: optimal and the result was: 615.7748831878779\n",
      "The status of solution was: optimal and the result was: 619.9572838047129\n",
      "The status of solution was: optimal and the result was: 660.5979447098946\n",
      "The status of solution was: optimal and the result was: 1023.5509386638963\n",
      "The status of solution was: optimal and the result was: 4623.550902346263\n",
      "Training with the best lambda: 10 on entire training set...\n",
      "The status of solution was: optimal and the result was: 75.03867170825443\n",
      "It took 3.60 minutes.\n",
      "\t\t\tError type:  mse\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 527.695472426736\n",
      "The status of solution was: optimal and the result was: 528.1679689566333\n",
      "The status of solution was: optimal and the result was: 532.4087398668854\n",
      "The status of solution was: optimal and the result was: 573.6432818175773\n",
      "The status of solution was: optimal and the result was: 937.7197627373182\n",
      "The status of solution was: optimal and the result was: 4537.719759395752\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 582.6098922009489\n",
      "The status of solution was: optimal and the result was: 583.0769548464941\n",
      "The status of solution was: optimal and the result was: 587.2692827405871\n",
      "The status of solution was: optimal and the result was: 628.0387376918154\n",
      "The status of solution was: optimal and the result was: 991.3514789334777\n",
      "The status of solution was: optimal and the result was: 4591.351435725568\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 615.3088193466681\n",
      "The status of solution was: optimal and the result was: 615.7748831878779\n",
      "The status of solution was: optimal and the result was: 619.9572838047129\n",
      "The status of solution was: optimal and the result was: 660.5979447098946\n",
      "The status of solution was: optimal and the result was: 1023.5509386638963\n",
      "The status of solution was: optimal and the result was: 4623.550902346263\n",
      "Training with the best lambda: 0 on entire training set...\n",
      "The status of solution was: optimal and the result was: 33.93614843351183\n",
      "It took 3.59 minutes.\n",
      "\t\t\tError type:  neg_corr\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 527.695472426736\n",
      "The status of solution was: optimal and the result was: 528.1679689566333\n",
      "The status of solution was: optimal and the result was: 532.4087398668854\n",
      "The status of solution was: optimal and the result was: 573.6432818175773\n",
      "The status of solution was: optimal and the result was: 937.7197627373182\n",
      "The status of solution was: optimal and the result was: 4537.719759395752\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 582.6098922009489\n",
      "The status of solution was: optimal and the result was: 583.0769548464941\n",
      "The status of solution was: optimal and the result was: 587.2692827405871\n",
      "The status of solution was: optimal and the result was: 628.0387376918154\n",
      "The status of solution was: optimal and the result was: 991.3514789334777\n",
      "The status of solution was: optimal and the result was: 4591.351435725568\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 615.3088193466681\n",
      "The status of solution was: optimal and the result was: 615.7748831878779\n",
      "The status of solution was: optimal and the result was: 619.9572838047129\n",
      "The status of solution was: optimal and the result was: 660.5979447098946\n",
      "The status of solution was: optimal and the result was: 1023.5509386638963\n",
      "The status of solution was: optimal and the result was: 4623.550902346263\n",
      "Training with the best lambda: 100 on entire training set...\n",
      "The status of solution was: optimal and the result was: 435.0386619777445\n",
      "It took 3.62 minutes.\n",
      "\t\t\tError type:  cosine_dist\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 527.695472426736\n",
      "The status of solution was: optimal and the result was: 528.1679689566333\n",
      "The status of solution was: optimal and the result was: 532.4087398668854\n",
      "The status of solution was: optimal and the result was: 573.6432818175773\n",
      "The status of solution was: optimal and the result was: 937.7197627373182\n",
      "The status of solution was: optimal and the result was: 4537.719759395752\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 582.6098922009489\n",
      "The status of solution was: optimal and the result was: 583.0769548464941\n",
      "The status of solution was: optimal and the result was: 587.2692827405871\n",
      "The status of solution was: optimal and the result was: 628.0387376918154\n",
      "The status of solution was: optimal and the result was: 991.3514789334777\n",
      "The status of solution was: optimal and the result was: 4591.351435725568\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 615.3088193466681\n",
      "The status of solution was: optimal and the result was: 615.7748831878779\n",
      "The status of solution was: optimal and the result was: 619.9572838047129\n",
      "The status of solution was: optimal and the result was: 660.5979447098946\n",
      "The status of solution was: optimal and the result was: 1023.5509386638963\n",
      "The status of solution was: optimal and the result was: 4623.550902346263\n",
      "Training with the best lambda: 1 on entire training set...\n",
      "The status of solution was: optimal and the result was: 38.61232606583476\n",
      "It took 3.60 minutes.\n",
      "\t\tModel:  concatinated_deep_neural_network_model_func ...\n",
      "\t\t\tError type:  normalized_frob_norm\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "Input size for the neural network was: 1\n",
      "It took 0.86 seconds.\n",
      "It took 14.44 minutes.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b66cf2ed38ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                             \u001b[0mlambdas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLAMBDAS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                             \u001b[0mmodel_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_model_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                             params={'with_constraints': True, 'n_splits': 3, 'n_epochs': 10, 'batch_size': 32})\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0mkey_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_type_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mselected_model_func_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     train_errors_in_runs[error_type_str][str(feature_names)][selected_model_func_str].append(\n",
      "\u001b[0;32m<ipython-input-6-aeb58f2f2ea3>\u001b[0m in \u001b[0;36mmodel_builder\u001b[0;34m(X_train, y_train, X_test, y_test, feature_names, estimation_name, error_type_str, tune_hyperparameters_by_validation, with_replication, lambdas, model_func, params)\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mlambdaa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambdaa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0merror_type_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     params=params)[1]\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mbest_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a94196ea77a1>\u001b[0m in \u001b[0;36mconcatinated_deep_neural_network_model_func\u001b[0;34m(X_train, y_train, X_validation_or_test, y_validation_or_test, feature_names, estimation_name, lambdaa, error_type_str, params)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#             activity_regularizer=regularizers.l1(lambdaa)),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#         Dropout(0.5),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         Dense(16, kernel_initializer=my_init, activation='softmax')])\n\u001b[0m\u001b[1;32m     69\u001b[0m     model.compile(optimizer='adam',\n\u001b[1;32m     70\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_init' is not defined"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "f = open('output{}.txt'.format(str(datetime.datetime.now())), 'w')\n",
    "sys.stdout = Tee(sys.stdout, f)\n",
    "\n",
    "with Timer():\n",
    "    train_errors_in_runs = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    test_errors_in_runs = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    validation_errors_in_runs = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    for run in range(RUNS):\n",
    "        print('Run', run + 1, '=>>>')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            np.array(data['X']), np.array(data['y']), test_size=TEST_FRACTION)\n",
    "        for feature_names in FEATURE_NAMES_SET:\n",
    "            print('\\tFeatures: ', feature_names, '...')\n",
    "            for selected_model_func in SELECTED_MODEL_FUNCS:\n",
    "                if isinstance(selected_model_func, str):\n",
    "                    selected_model_func_str = selected_model_func\n",
    "                else:\n",
    "                    selected_model_func_str = selected_model_func.__name__\n",
    "                print('\\t\\tModel: ', selected_model_func_str, '...')\n",
    "                for error_type_str in ERROR_TYPE_STRS:\n",
    "                    print('\\t\\t\\tError type: ', error_type_str)\n",
    "                    train_error = 0\n",
    "                    test_error = 0\n",
    "                    validation_errors = 0\n",
    "                    with Timer():\n",
    "                        train_error, test_error, validation_errors = model_builder(\n",
    "                            X_train=X_train,\n",
    "                            y_train=y_train,\n",
    "                            X_test=X_test,\n",
    "                            y_test=y_test,\n",
    "                            feature_names=feature_names,\n",
    "                            estimation_name=ESTIMATION_NAME,\n",
    "                            error_type_str=error_type_str,\n",
    "                            tune_hyperparameters_by_validation=TUNE_HYPERPARAMETERS_BY_VALIDATION,\n",
    "                            with_replication=WITH_REPLICATION,\n",
    "                            lambdas=LAMBDAS,\n",
    "                            model_func=selected_model_func,\n",
    "                            params={'with_constraints': True, 'n_splits': 3, 'n_epochs': 10, 'batch_size': 32})\n",
    "                    key_str = error_type_str + ':' + str(feature_names) + ':' + selected_model_func_str\n",
    "                    train_errors_in_runs[error_type_str][str(feature_names)][selected_model_func_str].append(\n",
    "                        train_error)\n",
    "                    test_errors_in_runs[error_type_str][str(feature_names)][selected_model_func_str].append(\n",
    "                        test_error)\n",
    "                    validation_errors_in_runs[error_type_str][str(feature_names)][selected_model_func_str].append(\n",
    "                        validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_it(train_errors_in_runs, 'train_errors.pkl')\n",
    "utils.save_it(test_errors_in_runs, 'test_errors_in_runs.pkl')\n",
    "utils.save_it(validation_errors_in_runs, 'validation_errors_in_runs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for setting_str, errors in test_errors_in_runs.items():\n",
    "#     print(setting_str, '=>> ', np.mean(errors), '+-', np.std(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting_str_list = []\n",
    "# for setting_str, errors in test_errors_in_runs.items():\n",
    "#     plt.hist(errors)\n",
    "#     setting_str_list.append(setting_str)\n",
    "# # plt.legend(setting_str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     np.array(data['X']), np.array(data['y']), test_size=TEST_FRACTION)\n",
    "# estimation_name = ESTIMATION_NAMES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a, b, c = model_builder(\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     feature_names=['content_embedding_matrix'],\n",
    "#     estimation_name=estimation_name,\n",
    "#     error_type_str='normalized_frob_norm',\n",
    "#     tune_hyperparameters_by_validation=False,\n",
    "#     with_replication=False,\n",
    "#     lambdas=LAMBDAS,\n",
    "#     model_func=convex_optimization_model_func,\n",
    "#     params={'with_constraints': True, 'n_splits': 3, 'n_epochs': 10, 'batch_size': 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
