{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# Full pipeline for the influence matrix estimation problem on the supervised dataset from the Jeopardy-like logs for comparison on different models\n",
    "# ===========================================================\n",
    "\n",
    "Goals:\n",
    "1. Split the data into test and train, and validation for multiple runs\n",
    "2. Formulate all different models of convex optimization, neural networks, and tower models.\n",
    "3. Give the same splits to all models, tune the hyperparameters with validation set, and report train and test erros as a pickle, a table, and a figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Started on: 30 Dec 2019\n",
    "#### Last update: 02 Jan 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import, unicode_literals\n",
    "\n",
    "import imp\n",
    "import sys\n",
    "import scipy as sp\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, LSTM, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Concatenate, Reshape, Embedding, Dot\n",
    "from tensorflow.keras.models import Model\n",
    "sys.path.insert(0, '../src/')\n",
    "%matplotlib inline\n",
    "\n",
    "import utils\n",
    "from mytimer import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = '/home/omid/Datasets/Jeopardy/supervised_data.pk'\n",
    "# DATA_FILE_PATH = '/home/omid/Datasets/Jeopardy/supervised_data_roberta.pk'\n",
    "TEST_FRACTION = 0.2\n",
    "RUNS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload():\n",
    "    imp.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_init(shape, dtype=None):\n",
    "    return np.ones(shape) * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_eigvec_of_laplacian(A: np.matrix) -> np.matrix:\n",
    "# #     D = np.diag(np.array(np.sum(A, axis=0))[0])\n",
    "# #     L = D - A\n",
    "# #     return np.matrix(np.linalg.eig(L)[1])\n",
    "#     n, m = A.shape\n",
    "#     diags = A.sum(axis=1).flatten()\n",
    "#     D = sp.sparse.spdiags(diags, [0], m, n, format='csr')\n",
    "#     L = D - A\n",
    "#     with sp.errstate(divide='ignore'):\n",
    "#         diags_sqrt = 1.0/sp.sqrt(diags)\n",
    "#     diags_sqrt[sp.isinf(diags_sqrt)] = 0\n",
    "#     DH = sp.sparse.spdiags(diags_sqrt, [0], m, n, format='csr')\n",
    "#     DH = DH.todense()\n",
    "#     normalized_L = DH.dot(L.dot(DH))\n",
    "#     return normalized_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comput_error(y_train_or_validation_or_test_true, y_train_or_validation_or_test_predicted, estimation_name, error_type_str):\n",
    "    \"\"\"Computes the error.\"\"\"\n",
    "    err = 0\n",
    "    for index in range(len(y_train_or_validation_or_test_true)):\n",
    "        groundtruth = y_train_or_validation_or_test_true[index][estimation_name]\n",
    "        predicted = y_train_or_validation_or_test_predicted[index]\n",
    "        # TODO(@Omid): CHECK WHETHER THIS IS JUST AN INT THEN FIND THE MOST INFLUENTIAL PERSON FROM THE ESTIAMTED MATRIX.\n",
    "        err += utils.matrix_estimation_error(\n",
    "            true_matrix=groundtruth, pred_matrix=predicted, type_str=error_type_str)\n",
    "    err /= len(y_train_or_validation_or_test_true)\n",
    "    return err\n",
    "\n",
    "\n",
    "def model_builder(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        feature_names,\n",
    "        estimation_name='influence_matrix',\n",
    "        error_type_str='normalized_frob_norm',\n",
    "        tune_hyperparameters_by_validation=True,\n",
    "        with_replication=True,\n",
    "        lambdas = [0, 0.1, 1, 10, 100, 1000],\n",
    "        model_func=convex_optimization_model_func,\n",
    "        params={'with_constraints': True, 'n_splits': 3}):\n",
    "    \n",
    "    # For the baseline models.\n",
    "    if model_func == 'average':\n",
    "        mats = []\n",
    "        for i in range(len(y_train)):\n",
    "            mats.append(y_train[i][estimation_name])\n",
    "        y_baseline_predicted = [np.matrix(np.mean(mats, axis=0)) for _ in range(len(y_train))]\n",
    "    elif model_func == 'uniform':\n",
    "        y_baseline_predicted = [np.matrix(np.ones((4, 4)) * 0.25) for _ in range(len(y_train))]\n",
    "    elif model_func == 'random':\n",
    "        y_baseline_predicted = [np.matrix(\n",
    "            utils.make_matrix_row_stochastic(\n",
    "                np.random.rand(4, 4))) for _ in range(len(y_train))]\n",
    "    if model_func in ['average', 'uniform', 'random']:\n",
    "        y_train_average = []\n",
    "        train_error = comput_error(\n",
    "            y_train, y_baseline_predicted, estimation_name=estimation_name, error_type_str=error_type_str)\n",
    "        test_error = comput_error(\n",
    "            y_test, y_baseline_predicted, estimation_name=estimation_name, error_type_str=error_type_str)\n",
    "        return train_error, test_error, _\n",
    "    \n",
    "    # For the proposed models.\n",
    "    validation_errors = defaultdict(lambda: 0)\n",
    "    if tune_hyperparameters_by_validation:\n",
    "        print('{}-fold validation ...'.format(params['n_splits']))\n",
    "        kf = KFold(n_splits=params['n_splits'])\n",
    "        for train_index, validation_index in kf.split(X_train):\n",
    "            X_train_subset, X_validation = X_train[train_index], X_train[validation_index]\n",
    "            y_train_subset, y_validation = y_train[train_index], y_train[validation_index]\n",
    "            if with_replication:\n",
    "                X_train_subset, y_train_subset = utils.replicate_matrices_in_train_dataset_with_reordering(\n",
    "                    X_train_subset, y_train_subset)\n",
    "                X_train_subset = np.array(X_train_subset)\n",
    "                y_train_subset = np.array(y_train_subset)\n",
    "            print('Shapes of train: {}, validation: {}, test: {}.'.format(\n",
    "                X_train_subset.shape, X_validation.shape, X_test.shape))\n",
    "            for lambdaa in lambdas:\n",
    "                validation_errors[lambdaa] += model_func(\n",
    "                    X_train=X_train_subset,\n",
    "                    y_train=y_train_subset,\n",
    "                    X_validation_or_test=X_validation,\n",
    "                    y_validation_or_test=y_validation,\n",
    "                    feature_names=feature_names,\n",
    "                    estimation_name=estimation_name,\n",
    "                    lambdaa=lambdaa,\n",
    "                    error_type_str=error_type_str,\n",
    "                    params=params)[1]\n",
    "        best_lambda = min(validation_errors, key=validation_errors.get)\n",
    "    else:\n",
    "        best_lambda = 0.1\n",
    "    print('Training with the best lambda: {} on entire training set...'.format(best_lambda))\n",
    "    train_error, test_error = model_func(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_validation_or_test=X_test,\n",
    "        y_validation_or_test=y_test,\n",
    "        feature_names=feature_names,\n",
    "        estimation_name=estimation_name,\n",
    "        lambdaa=best_lambda,\n",
    "        error_type_str=error_type_str,\n",
    "        params=params)\n",
    "    return train_error, test_error, validation_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convex_optimization_model_func(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation_or_test,\n",
    "        y_validation_or_test,\n",
    "        feature_names,\n",
    "        estimation_name,\n",
    "        lambdaa,\n",
    "        error_type_str,\n",
    "        params={'with_constraints': True}):\n",
    "    \n",
    "    def predict(data_element, feature_names, B, Ws, is_solved=False):\n",
    "        \"\"\"Defines the prediction function.\"\"\"\n",
    "        predicted = 0\n",
    "        if is_solved:\n",
    "            predicted += B.value\n",
    "        else:\n",
    "            predicted += B\n",
    "        for feature_name in feature_names:\n",
    "            if is_solved:\n",
    "                W_for_this_feature = Ws[feature_name].value\n",
    "            else:\n",
    "                W_for_this_feature = Ws[feature_name]\n",
    "            if len(data_element[feature_name].shape) == 1:\n",
    "                # If it was a vector, it makes a matrix with it.\n",
    "                p = data_element[feature_name]\n",
    "                data_element_matrix = np.row_stack([p, p, p, p])\n",
    "            else:\n",
    "                # Unless it is already a matrix.\n",
    "                data_element_matrix = data_element[feature_name]\n",
    "            predicted += (W_for_this_feature * data_element_matrix)\n",
    "        return predicted\n",
    "    \n",
    "    def predict_all_after_solving(X_train_or_validation_or_test, B, Ws, feature_names):\n",
    "        \"\"\"Predicts for all data points in the given set.\"\"\"\n",
    "        return [\n",
    "            predict(\n",
    "                data_element=data_element,\n",
    "                feature_names=feature_names,\n",
    "                B=B,\n",
    "                Ws=Ws,\n",
    "                is_solved=True) \n",
    "            for data_element in X_train_or_validation_or_test]\n",
    "\n",
    "    # Creating variables.\n",
    "    Ws = {}\n",
    "    for feature_name in feature_names:\n",
    "        if len(X_train[0][feature_name].shape) == 1:\n",
    "            # If it was a vector, it makes a matrix with it.\n",
    "            Ws[feature_name] = cp.Variable(\n",
    "                len(X_train[0][feature_name]), len(X_train[0][feature_name]))\n",
    "        else:\n",
    "            # Unless it is already a matrix.\n",
    "            Ws[feature_name] = cp.Variable(\n",
    "                X_train[0][feature_name].shape[0], X_train[0][feature_name].shape[1])\n",
    "    B = cp.Variable(4, 4)\n",
    "\n",
    "    # Computing loss.\n",
    "    constraints = []\n",
    "    losses = 0\n",
    "    for index in range(len(X_train)):\n",
    "        element = X_train[index]\n",
    "        estimation_groundtruth = y_train[index][estimation_name]\n",
    "\n",
    "        # Defining the estimation function.\n",
    "        estimation_predicted = predict(\n",
    "            data_element=element, feature_names=feature_names, B=B, Ws=Ws, is_solved=False)\n",
    "\n",
    "        # Defining the loss function.\n",
    "        loss = cp.sum_squares(estimation_predicted - estimation_groundtruth)\n",
    "\n",
    "        losses += loss\n",
    "        if params['with_constraints']:\n",
    "            constraints += [estimation_predicted >= 0]\n",
    "            constraints += [cp.sum_entries(estimation_predicted, axis=1) == 1]\n",
    "\n",
    "    # Computing regularization.\n",
    "    regluarization = cp.norm1(B)\n",
    "    for feature_name in feature_names:\n",
    "        regluarization += cp.norm1(Ws[feature_name])\n",
    "\n",
    "    # Solving the convex problem.\n",
    "    objective = cp.Minimize(losses + lambdaa * regluarization)\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    result = prob.solve(solver=cp.MOSEK)\n",
    "    print('The status of solution was: {} and the result was: {}'.format(prob.status, result))\n",
    "\n",
    "    # Predicting and computing trian error.\n",
    "    y_train_predicted = predict_all_after_solving(\n",
    "        X_train_or_validation_or_test=X_train, B=B, Ws=Ws, feature_names=feature_names)\n",
    "    train_error = comput_error(\n",
    "        y_train_or_validation_or_test_true=y_train,\n",
    "        y_train_or_validation_or_test_predicted=y_train_predicted,\n",
    "        estimation_name=estimation_name,\n",
    "        error_type_str=error_type_str)\n",
    "    \n",
    "    # Predicting and computing validation or test error.\n",
    "    y_validation_or_test_predicted = predict_all_after_solving(\n",
    "        X_train_or_validation_or_test=X_validation_or_test, B=B, Ws=Ws, feature_names=feature_names)\n",
    "    validation_or_test_error = comput_error(\n",
    "        y_train_or_validation_or_test_true=y_validation_or_test,\n",
    "        y_train_or_validation_or_test_predicted=y_validation_or_test_predicted,\n",
    "        estimation_name=estimation_name,\n",
    "        error_type_str=error_type_str)\n",
    "    \n",
    "    return train_error, validation_or_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatinated_deep_neural_network_model_func(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation_or_test,\n",
    "        y_validation_or_test,\n",
    "        feature_names,\n",
    "        estimation_name,\n",
    "        lambdaa,\n",
    "        error_type_str,\n",
    "        params={'n_epochs': 10, 'batch_size': 32}):\n",
    "    \n",
    "#     def predict(data_element, feature_names, B, Ws, is_solved=False):\n",
    "#         \"\"\"Defines the prediction function.\"\"\"\n",
    "#         return 0\n",
    "    \n",
    "#     def predict_all_after_solving(X_train_or_validation_or_test, B, Ws, feature_names):\n",
    "#         \"\"\"Predicts for all data points in the given set.\"\"\"\n",
    "#         return []\n",
    "\n",
    "    flatten_X_train = []\n",
    "    flatten_y_train = []\n",
    "    for i in range(len(X_train)):\n",
    "        features = X_train[i]\n",
    "        label = y_train[i][estimation_name]            \n",
    "        flatten_X_train.append(np.hstack(\n",
    "            [np.array(features[feature_name].flatten())[0] for feature_name in feature_names]))\n",
    "        flatten_y_train.append(np.array(label.flatten())[0])\n",
    "    flatten_X_train = np.array(flatten_X_train)\n",
    "    flatten_y_train = np.array(flatten_y_train)\n",
    "\n",
    "    flatten_X_validation_or_test = []\n",
    "    flatten_y_validation_or_test = []\n",
    "    for i in range(len(X_validation_or_test)):\n",
    "        features = X_validation_or_test[i]\n",
    "        label = y_validation_or_test[i][estimation_name]\n",
    "        flatten_X_validation_or_test.append(np.hstack(\n",
    "            [np.array(features[feature_name].flatten())[0] for feature_name in feature_names]))\n",
    "        flatten_y_validation_or_test.append(np.array(label.flatten())[0])\n",
    "    flatten_X_validation_or_test = np.array(flatten_X_validation_or_test)\n",
    "    flatten_y_validation_or_test = np.array(flatten_y_validation_or_test)\n",
    "                              \n",
    "    _, input_size = flatten_X_train.shape\n",
    "    print('Input size for the neural network was: {}'.format(input_size))\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(\n",
    "            units=32,\n",
    "            kernel_initializer='he_normal',\n",
    "            activation='relu',\n",
    "            input_shape=(input_size,),\n",
    "            kernel_regularizer=regularizers.l1(lambdaa),\n",
    "            activity_regularizer=regularizers.l1(lambdaa)),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(\n",
    "#             units=64,\n",
    "#             kernel_initializer='he_normal',\n",
    "#             activation='relu',\n",
    "#             kernel_regularizer=regularizers.l1(lambdaa),\n",
    "#             activity_regularizer=regularizers.l1(lambdaa)),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(\n",
    "#             units=32,\n",
    "#             kernel_initializer='he_normal',\n",
    "#             activation='relu',\n",
    "#             kernel_regularizer=regularizers.l1(lambdaa),\n",
    "#             activity_regularizer=regularizers.l1(lambdaa)),\n",
    "#         Dropout(0.5),\n",
    "        Dense(16, kernel_initializer=my_init, activation='softmax')])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(flatten_X_train, flatten_y_train, epochs=params['n_epochs'], batch_size=params['batch_size'])\n",
    "\n",
    "    # Predicting and computing trian error.\n",
    "    y_train_predicted = [utils.make_matrix_row_stochastic(\n",
    "        np.matrix(np.reshape(element, (4, 4)))) for element in model.predict(flatten_X_train)]\n",
    "    train_error = comput_error(\n",
    "        y_train_or_validation_or_test_true=y_train,\n",
    "        y_train_or_validation_or_test_predicted=y_train_predicted,\n",
    "        estimation_name=estimation_name,\n",
    "        error_type_str=error_type_str)\n",
    "                                            \n",
    "    # Predicting and computing trian error.\n",
    "    y_test_predicted = [utils.make_matrix_row_stochastic(\n",
    "        np.matrix(np.reshape(element, (4, 4)))) for element in model.predict(flatten_X_validation_or_test)]\n",
    "    validation_or_test_error = comput_error(\n",
    "        y_train_or_validation_or_test_true=y_test,\n",
    "        y_train_or_validation_or_test_predicted=y_test_predicted,\n",
    "        estimation_name=estimation_name,\n",
    "        error_type_str=error_type_str)\n",
    "\n",
    "    return train_error, validation_or_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "data = utils.load_it(DATA_FILE_PATH)\n",
    "print(len(data['X']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just to see how much headroom there is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25784649, 0.25013211, 0.25917224, 0.23284916],\n",
       "       [0.21997335, 0.3244428 , 0.22187468, 0.23370918],\n",
       "       [0.21050033, 0.25039752, 0.31083892, 0.22826324],\n",
       "       [0.25180346, 0.23839655, 0.24293584, 0.26686416]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mats = []\n",
    "for i in range(len(data['y'])):\n",
    "    mats.append(data['y'][i]['influence_matrix'])\n",
    "np.mean(mats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25784649, 0.25013211, 0.25917224, 0.23284916],\n",
       "       [0.21997335, 0.3244428 , 0.22187468, 0.23370918],\n",
       "       [0.21050033, 0.25039752, 0.31083892, 0.22826324],\n",
       "       [0.25180346, 0.23839655, 0.24293584, 0.26686416]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11677549, 0.08129962, 0.08962429, 0.07536874],\n",
       "       [0.08929333, 0.16587926, 0.08335618, 0.10916787],\n",
       "       [0.08963767, 0.12213777, 0.16865608, 0.09193362],\n",
       "       [0.09597464, 0.06509444, 0.05915974, 0.08748726]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(mats, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message: It seems the std is small, and average is close to 0.25 everywhere but main diagonal which is slightly larger (due to having selfish people)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['average_of_previous_influence_matrices', 'content_embedding_matrix', 'individual_performance_hardness_weighted', 'emotion_valence', 'emotion_dominance', 'first_influence_matrix', 'individual_performance', 'sentiment', 'emotion_arousal', 'reply_duration'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['X'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDAS = [0, 0.1, 1, 10, 100, 1000]\n",
    "WITH_REPLICATION = True\n",
    "ERROR_TYPE_STRS = ['normalized_frob_norm', 'mse', 'neg_corr', 'cosine_dist']\n",
    "TUNE_HYPERPARAMETERS_BY_VALIDATION = True\n",
    "SELECTED_MODEL_FUNCS = ['average', 'uniform', 'random', convex_optimization_model_func, concatinated_deep_neural_network_model_func]\n",
    "FEATURE_NAMES_SET = [['individual_performance'], ['first_influence_matrix'], ['reply_duration'], ['content_embedding_matrix'], ['average_of_previous_influence_matrices']]\n",
    "ESTIMATION_NAMES = ['influence_matrix', 'most_influentials']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation comparison pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 =>>>\n",
      "Features:  ['individual_performance'] ...\n",
      "Model:  average ...\n",
      "It took 0.05 seconds.\n",
      "Model:  uniform ...\n",
      "It took 0.01 seconds.\n",
      "Model:  random ...\n",
      "It took 0.03 seconds.\n",
      "Model:  <function convex_optimization_model_func at 0x7f4d855e48c8> ...\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 583.1121941681871\n",
      "The status of solution was: optimal and the result was: 583.512192508478\n",
      "The status of solution was: optimal and the result was: 587.1121940821129\n",
      "The status of solution was: optimal and the result was: 623.1121996097442\n",
      "The status of solution was: optimal and the result was: 983.1121791699754\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 598.8717429761807\n",
      "The status of solution was: optimal and the result was: 599.2717523963174\n",
      "The status of solution was: optimal and the result was: 602.8717441264012\n",
      "The status of solution was: optimal and the result was: 638.8717519902057\n",
      "The status of solution was: optimal and the result was: 998.871739612525\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 561.7642878442272\n",
      "The status of solution was: optimal and the result was: 562.1642945636265\n",
      "The status of solution was: optimal and the result was: 565.764288430431\n",
      "The status of solution was: optimal and the result was: 601.7642944901645\n",
      "The status of solution was: optimal and the result was: 961.7642843835663\n",
      "Training with the best lambda: 10 on entire training set...\n",
      "The status of solution was: optimal and the result was: 75.18124324905759\n",
      "It took 2.83 minutes.\n",
      "Features:  ['first_influence_matrix'] ...\n",
      "Model:  average ...\n",
      "It took 0.05 seconds.\n",
      "Model:  uniform ...\n",
      "It took 0.01 seconds.\n",
      "Model:  random ...\n",
      "It took 0.02 seconds.\n",
      "Model:  <function convex_optimization_model_func at 0x7f4d855e48c8> ...\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 476.10060684368284\n",
      "The status of solution was: optimal and the result was: 476.5006070248934\n",
      "The status of solution was: optimal and the result was: 480.10060668975666\n",
      "The status of solution was: optimal and the result was: 516.1006023892824\n",
      "The status of solution was: optimal and the result was: 876.1005966136628\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 504.734757829174\n",
      "The status of solution was: optimal and the result was: 505.134757962482\n",
      "The status of solution was: optimal and the result was: 508.7347576393848\n",
      "The status of solution was: optimal and the result was: 544.7347537884996\n",
      "The status of solution was: optimal and the result was: 904.7347493193417\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 468.8665427455411\n",
      "The status of solution was: optimal and the result was: 469.2665429262393\n",
      "The status of solution was: optimal and the result was: 472.8665427220765\n",
      "The status of solution was: optimal and the result was: 508.8665389311675\n",
      "The status of solution was: optimal and the result was: 868.8665376412256\n",
      "Training with the best lambda: 10 on entire training set...\n",
      "The status of solution was: optimal and the result was: 69.28250423714002\n",
      "It took 2.91 minutes.\n",
      "Features:  ['average_of_previous_influence_matrices'] ...\n",
      "Model:  average ...\n",
      "It took 0.05 seconds.\n",
      "Model:  uniform ...\n",
      "It took 0.01 seconds.\n",
      "Model:  random ...\n",
      "It took 0.02 seconds.\n",
      "Model:  <function convex_optimization_model_func at 0x7f4d855e48c8> ...\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 290.5489640970393\n",
      "The status of solution was: optimal and the result was: 290.9489640219856\n",
      "The status of solution was: optimal and the result was: 294.54896334661555\n",
      "The status of solution was: optimal and the result was: 330.54896655725315\n",
      "The status of solution was: optimal and the result was: 690.5489557163062\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 335.174856064525\n",
      "The status of solution was: optimal and the result was: 335.5748550790432\n",
      "The status of solution was: optimal and the result was: 339.17485957473076\n",
      "The status of solution was: optimal and the result was: 375.1748556514534\n",
      "The status of solution was: optimal and the result was: 735.1748498272563\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 314.84819442768645\n",
      "The status of solution was: optimal and the result was: 315.24819339143045\n",
      "The status of solution was: optimal and the result was: 318.8481982726511\n",
      "The status of solution was: optimal and the result was: 354.8481936234223\n",
      "The status of solution was: optimal and the result was: 714.8481910118065\n",
      "Training with the best lambda: 10 on entire training set...\n",
      "The status of solution was: optimal and the result was: 59.06225646674916\n",
      "It took 3.00 minutes.\n",
      "Run 2 =>>>\n",
      "Features:  ['individual_performance'] ...\n",
      "Model:  average ...\n",
      "It took 0.04 seconds.\n",
      "Model:  uniform ...\n",
      "It took 0.01 seconds.\n",
      "Model:  random ...\n",
      "It took 0.02 seconds.\n",
      "Model:  <function convex_optimization_model_func at 0x7f4d855e48c8> ...\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 604.9104561085746\n",
      "The status of solution was: optimal and the result was: 605.3104543821731\n",
      "The status of solution was: optimal and the result was: 608.9104560900732\n",
      "The status of solution was: optimal and the result was: 644.9104605742231\n",
      "The status of solution was: optimal and the result was: 1004.910456237777\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 596.4410274780055\n",
      "The status of solution was: optimal and the result was: 596.8410345526088\n",
      "The status of solution was: optimal and the result was: 600.4410278784424\n",
      "The status of solution was: optimal and the result was: 636.4410342072878\n",
      "The status of solution was: optimal and the result was: 996.4410229394907\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 614.1820991828256\n",
      "The status of solution was: optimal and the result was: 614.5821076352427\n",
      "The status of solution was: optimal and the result was: 618.1820999517511\n",
      "The status of solution was: optimal and the result was: 654.1821071965039\n",
      "The status of solution was: optimal and the result was: 1014.1820878853982\n",
      "Training with the best lambda: 10 on entire training set...\n",
      "The status of solution was: optimal and the result was: 76.61038120789367\n",
      "It took 2.85 minutes.\n",
      "Features:  ['first_influence_matrix'] ...\n",
      "Model:  average ...\n",
      "It took 0.05 seconds.\n",
      "Model:  uniform ...\n",
      "It took 0.01 seconds.\n",
      "Model:  random ...\n",
      "It took 0.02 seconds.\n",
      "Model:  <function convex_optimization_model_func at 0x7f4d855e48c8> ...\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 514.2460370280382\n",
      "The status of solution was: optimal and the result was: 514.6460295707911\n",
      "The status of solution was: optimal and the result was: 518.2460295723284\n",
      "The status of solution was: optimal and the result was: 554.2460327933469\n",
      "The status of solution was: optimal and the result was: 914.2460313147297\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 479.72719566327027\n",
      "The status of solution was: optimal and the result was: 480.12719591736425\n",
      "The status of solution was: optimal and the result was: 483.7271956240985\n",
      "The status of solution was: optimal and the result was: 519.7271905336074\n",
      "The status of solution was: optimal and the result was: 879.7271859943944\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 499.1487476692696\n",
      "The status of solution was: optimal and the result was: 499.54874574347616\n",
      "The status of solution was: optimal and the result was: 503.14874762621054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The status of solution was: optimal and the result was: 539.1487446717288\n",
      "The status of solution was: optimal and the result was: 899.1487402733535\n",
      "Training with the best lambda: 10 on entire training set...\n",
      "The status of solution was: optimal and the result was: 69.81939097583108\n",
      "It took 2.89 minutes.\n",
      "Features:  ['average_of_previous_influence_matrices'] ...\n",
      "Model:  average ...\n",
      "It took 0.06 seconds.\n",
      "Model:  uniform ...\n",
      "It took 0.01 seconds.\n",
      "Model:  random ...\n",
      "It took 0.02 seconds.\n",
      "Model:  <function convex_optimization_model_func at 0x7f4d855e48c8> ...\n",
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 339.93614073315484\n",
      "The status of solution was: optimal and the result was: 340.3361416384775\n",
      "The status of solution was: optimal and the result was: 343.9361332164393\n",
      "The status of solution was: optimal and the result was: 379.9361389258591\n",
      "The status of solution was: optimal and the result was: 739.9361149895034\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 326.6473195373334\n",
      "The status of solution was: optimal and the result was: 327.0473182043834\n",
      "The status of solution was: optimal and the result was: 330.6473209787875\n",
      "The status of solution was: optimal and the result was: 366.64731524412343\n",
      "The status of solution was: optimal and the result was: 726.6473084148438\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "The status of solution was: optimal and the result was: 360.09103235648405\n",
      "The status of solution was: optimal and the result was: 360.49103030187405\n",
      "The status of solution was: optimal and the result was: 364.09102586981714\n",
      "The status of solution was: optimal and the result was: 400.0910218896554\n",
      "The status of solution was: optimal and the result was: 760.0910176453469\n",
      "Training with the best lambda: 0 on entire training set...\n",
      "The status of solution was: optimal and the result was: 20.71722162679722\n",
      "It took 2.97 minutes.\n",
      "It took 17.46 minutes.\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    estimation_name = ESTIMATION_NAMES[0]\n",
    "    error_type_str = ERROR_TYPE_STRS[0]\n",
    "    \n",
    "    train_errors_in_runs = defaultdict(list)\n",
    "    test_errors_in_runs = defaultdict(list)\n",
    "    validation_errors_in_runs = defaultdict(list)\n",
    "    for run in range(RUNS):\n",
    "        print('Run', run + 1, '=>>>')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            np.array(data['X']), np.array(data['y']), test_size=TEST_FRACTION)\n",
    "        for feature_names in FEATURE_NAMES_SET:\n",
    "            print('Features: ', feature_names, '...')\n",
    "            for selected_model_func in SELECTED_MODEL_FUNCS:\n",
    "                print('Model: ', selected_model_func, '...')\n",
    "                with Timer():\n",
    "                    train_error, test_error, validation_errors = model_builder(\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test,\n",
    "                        y_test=y_test,\n",
    "                        feature_names=feature_names,\n",
    "                        estimation_name=estimation_name,\n",
    "                        error_type_str=error_type_str,\n",
    "                        tune_hyperparameters_by_validation=TUNE_HYPERPARAMETERS_BY_VALIDATION,\n",
    "                        with_replication=WITH_REPLICATION,\n",
    "                        lambdas=LAMBDAS,\n",
    "                        model_func=selected_model_func,\n",
    "                        params={'with_constraints': True, 'n_splits': 3, 'n_epochs': 10, 'batch_size': 32})\n",
    "                key_str = str(selected_model_func) + ':' + str(feature_names)\n",
    "                train_errors_in_runs[key_str].append(train_error)\n",
    "                test_errors_in_runs[key_str].append(test_error)\n",
    "                validation_errors_in_runs[key_str].append(validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average:['average_of_previous_influence_matrices'] =>>  0.34364980214151475 +- 0.010812181527379783\n",
      "uniform:['average_of_previous_influence_matrices'] =>>  0.3495383349306685 +- 0.010645381369812412\n",
      "<function convex_optimization_model_func at 0x7f4d855e48c8>:['average_of_previous_influence_matrices'] =>>  0.2442421771438104 +- 0.01317857215832012\n",
      "average:['individual_performance'] =>>  0.34364980214151475 +- 0.010812181527379783\n",
      "<function convex_optimization_model_func at 0x7f4d855e48c8>:['individual_performance'] =>>  0.3436467014985325 +- 0.010812216276520847\n",
      "random:['first_influence_matrix'] =>>  0.6410311027354627 +- 0.016657903363658855\n",
      "uniform:['first_influence_matrix'] =>>  0.3495383349306685 +- 0.010645381369812412\n",
      "random:['average_of_previous_influence_matrices'] =>>  0.6289606832203576 +- 0.012499024868696584\n",
      "uniform:['individual_performance'] =>>  0.3495383349306685 +- 0.010645381369812412\n",
      "average:['first_influence_matrix'] =>>  0.34364980214151475 +- 0.010812181527379783\n",
      "<function convex_optimization_model_func at 0x7f4d855e48c8>:['first_influence_matrix'] =>>  0.3052897239077915 +- 0.006033903202813745\n",
      "random:['individual_performance'] =>>  0.618153263665356 +- 0.0023528337708834113\n"
     ]
    }
   ],
   "source": [
    "for setting_str, errors in test_errors_in_runs.items():\n",
    "    print(setting_str, '=>> ', np.mean(errors), '+-', np.std(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     np.array(data['X']), np.array(data['y']), test_size=TEST_FRACTION)\n",
    "# estimation_name = ESTIMATION_NAMES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-fold validation ...\n",
      "Shapes of train: (3360,), validation: (71,), test: (53,).\n",
      "Input size for the neural network was: 16\n",
      "Train on 3360 samples\n",
      "Epoch 1/10\n",
      "3360/3360 [==============================] - 1s 153us/sample - loss: 0.7392 - accuracy: 0.0094\n",
      "Epoch 2/10\n",
      "3360/3360 [==============================] - 0s 44us/sample - loss: 0.7362 - accuracy: 0.0094\n",
      "Epoch 3/10\n",
      "3360/3360 [==============================] - 0s 33us/sample - loss: 0.7353 - accuracy: 0.0094\n",
      "Epoch 4/10\n",
      "3360/3360 [==============================] - 0s 32us/sample - loss: 0.7351 - accuracy: 0.0094\n",
      "Epoch 5/10\n",
      "3360/3360 [==============================] - 0s 32us/sample - loss: 0.7351 - accuracy: 0.0094\n",
      "Epoch 6/10\n",
      "3360/3360 [==============================] - 0s 32us/sample - loss: 0.7350 - accuracy: 0.0094\n",
      "Epoch 7/10\n",
      "3360/3360 [==============================] - 0s 31us/sample - loss: 0.7349 - accuracy: 0.0094\n",
      "Epoch 8/10\n",
      "3360/3360 [==============================] - 0s 32us/sample - loss: 0.7349 - accuracy: 0.0094\n",
      "Epoch 9/10\n",
      "3360/3360 [==============================] - 0s 35us/sample - loss: 0.7349 - accuracy: 0.0094\n",
      "Epoch 10/10\n",
      "3360/3360 [==============================] - 0s 41us/sample - loss: 0.7348 - accuracy: 0.0094\n",
      "Input size for the neural network was: 16\n",
      "Train on 3360 samples\n",
      "Epoch 1/10\n",
      "3360/3360 [==============================] - 1s 258us/sample - loss: 13.6208 - accuracy: 0.0094\n",
      "Epoch 2/10\n",
      "3360/3360 [==============================] - 0s 42us/sample - loss: 9.1718 - accuracy: 0.0094\n",
      "Epoch 3/10\n",
      "3360/3360 [==============================] - 0s 35us/sample - loss: 5.8860 - accuracy: 0.0094\n",
      "Epoch 4/10\n",
      "3360/3360 [==============================] - 0s 37us/sample - loss: 3.6442 - accuracy: 0.0094\n",
      "Epoch 5/10\n",
      "3360/3360 [==============================] - 0s 36us/sample - loss: 2.2433 - accuracy: 0.0094\n",
      "Epoch 6/10\n",
      "3360/3360 [==============================] - 0s 37us/sample - loss: 1.4145 - accuracy: 0.0094\n",
      "Epoch 7/10\n",
      "3360/3360 [==============================] - 0s 35us/sample - loss: 0.9572 - accuracy: 0.0094\n",
      "Epoch 8/10\n",
      "3360/3360 [==============================] - 0s 35us/sample - loss: 0.7767 - accuracy: 0.0094\n",
      "Epoch 9/10\n",
      "3360/3360 [==============================] - 0s 37us/sample - loss: 0.7467 - accuracy: 0.0094\n",
      "Epoch 10/10\n",
      "3360/3360 [==============================] - 0s 36us/sample - loss: 0.7463 - accuracy: 0.0094\n",
      "Input size for the neural network was: 16\n",
      "Train on 3360 samples\n",
      "Epoch 1/10\n",
      "3360/3360 [==============================] - 1s 188us/sample - loss: 135.9877 - accuracy: 0.0094\n",
      "Epoch 2/10\n",
      "3360/3360 [==============================] - 0s 38us/sample - loss: 90.9677 - accuracy: 0.0094\n",
      "Epoch 3/10\n",
      "3360/3360 [==============================] - 0s 35us/sample - loss: 56.3433 - accuracy: 0.0094\n",
      "Epoch 4/10\n",
      "3360/3360 [==============================] - 0s 38us/sample - loss: 32.5246 - accuracy: 0.0094\n",
      "Epoch 5/10\n",
      "3360/3360 [==============================] - 0s 38us/sample - loss: 16.9271 - accuracy: 0.0094\n",
      "Epoch 6/10\n",
      "3360/3360 [==============================] - 0s 38us/sample - loss: 7.2626 - accuracy: 0.0094\n",
      "Epoch 7/10\n",
      "3360/3360 [==============================] - 0s 37us/sample - loss: 2.5447 - accuracy: 0.0094\n",
      "Epoch 8/10\n",
      "3360/3360 [==============================] - 0s 35us/sample - loss: 0.9640 - accuracy: 0.0094\n",
      "Epoch 9/10\n",
      "3360/3360 [==============================] - 0s 35us/sample - loss: 0.8008 - accuracy: 0.0094\n",
      "Epoch 10/10\n",
      "3360/3360 [==============================] - 0s 34us/sample - loss: 0.8005 - accuracy: 0.0094\n",
      "Input size for the neural network was: 16\n",
      "Train on 3360 samples\n",
      "Epoch 1/10\n",
      "3360/3360 [==============================] - 1s 251us/sample - loss: 1235.9257 - accuracy: 0.0094\n",
      "Epoch 2/10\n",
      "3360/3360 [==============================] - 0s 37us/sample - loss: 800.6103 - accuracy: 0.0094\n",
      "Epoch 3/10\n",
      "3360/3360 [==============================] - 0s 33us/sample - loss: 489.3269 - accuracy: 0.0094\n",
      "Epoch 4/10\n",
      "3360/3360 [==============================] - 0s 33us/sample - loss: 275.3807 - accuracy: 0.0094\n",
      "Epoch 5/10\n",
      "3360/3360 [==============================] - 0s 34us/sample - loss: 142.2118 - accuracy: 0.0094\n",
      "Epoch 6/10\n",
      "3360/3360 [==============================] - 0s 33us/sample - loss: 60.3189 - accuracy: 0.0094\n",
      "Epoch 7/10\n",
      "3360/3360 [==============================] - 0s 33us/sample - loss: 18.3509 - accuracy: 0.0094\n",
      "Epoch 8/10\n",
      "3360/3360 [==============================] - 0s 32us/sample - loss: 3.4427 - accuracy: 0.0094\n",
      "Epoch 9/10\n",
      "3360/3360 [==============================] - 0s 37us/sample - loss: 1.3363 - accuracy: 0.0094\n",
      "Epoch 10/10\n",
      "3360/3360 [==============================] - 0s 34us/sample - loss: 1.3335 - accuracy: 0.0094\n",
      "Input size for the neural network was: 16\n",
      "Train on 3360 samples\n",
      "Epoch 1/10\n",
      "3360/3360 [==============================] - 1s 162us/sample - loss: 12843.5443 - accuracy: 0.0094\n",
      "Epoch 2/10\n",
      "3360/3360 [==============================] - 0s 40us/sample - loss: 8413.8393 - accuracy: 0.0094\n",
      "Epoch 3/10\n",
      "3360/3360 [==============================] - 0s 37us/sample - loss: 5173.5943 - accuracy: 0.0094\n",
      "Epoch 4/10\n",
      "3360/3360 [==============================] - 0s 34us/sample - loss: 2959.8172 - accuracy: 0.0094\n",
      "Epoch 5/10\n",
      "3360/3360 [==============================] - 0s 33us/sample - loss: 1547.1418 - accuracy: 0.0094\n",
      "Epoch 6/10\n",
      "3360/3360 [==============================] - 0s 35us/sample - loss: 700.5122 - accuracy: 0.0094\n",
      "Epoch 7/10\n",
      "3360/3360 [==============================] - 0s 33us/sample - loss: 214.7584 - accuracy: 0.0094\n",
      "Epoch 8/10\n",
      "3360/3360 [==============================] - 0s 39us/sample - loss: 33.2705 - accuracy: 0.0094\n",
      "Epoch 9/10\n",
      "3360/3360 [==============================] - 0s 42us/sample - loss: 6.7879 - accuracy: 0.0094\n",
      "Epoch 10/10\n",
      "3360/3360 [==============================] - 0s 39us/sample - loss: 6.6772 - accuracy: 0.0094\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 165us/sample - loss: 0.7394 - accuracy: 0.0124\n",
      "Epoch 2/10\n",
      "3384/3384 [==============================] - 0s 44us/sample - loss: 0.7374 - accuracy: 0.0124\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 0.7368 - accuracy: 0.0124\n",
      "Epoch 4/10\n",
      "3384/3384 [==============================] - 0s 35us/sample - loss: 0.7366 - accuracy: 0.0124\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 34us/sample - loss: 0.7365 - accuracy: 0.0124\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 34us/sample - loss: 0.7364 - accuracy: 0.0124\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 34us/sample - loss: 0.7363 - accuracy: 0.0124\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 36us/sample - loss: 0.7363 - accuracy: 0.0124\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 0.7362 - accuracy: 0.0124\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 41us/sample - loss: 0.7362 - accuracy: 0.0124\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 170us/sample - loss: 13.4045 - accuracy: 0.0124\n",
      "Epoch 2/10\n",
      "3384/3384 [==============================] - 0s 35us/sample - loss: 8.9327 - accuracy: 0.0124\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 35us/sample - loss: 5.6958 - accuracy: 0.0124\n",
      "Epoch 4/10\n",
      "3384/3384 [==============================] - 0s 35us/sample - loss: 3.4738 - accuracy: 0.0124\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 36us/sample - loss: 2.0991 - accuracy: 0.0124\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 34us/sample - loss: 1.3300 - accuracy: 0.0124\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 35us/sample - loss: 0.9314 - accuracy: 0.0124\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 0.7740 - accuracy: 0.0124\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 0.7466 - accuracy: 0.0124\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 37us/sample - loss: 0.7465 - accuracy: 0.0124\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 198us/sample - loss: 134.8879 - accuracy: 0.0124\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3384/3384 [==============================] - 0s 44us/sample - loss: 87.7238 - accuracy: 0.0124\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 45us/sample - loss: 53.5002 - accuracy: 0.0124\n",
      "Epoch 4/10\n",
      "3384/3384 [==============================] - 0s 42us/sample - loss: 30.3800 - accuracy: 0.0124\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 15.5570 - accuracy: 0.0124\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 7.1421 - accuracy: 0.0124\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 2.7150 - accuracy: 0.0124\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 1.0535 - accuracy: 0.0124\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 41us/sample - loss: 0.7985 - accuracy: 0.0124\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 43us/sample - loss: 0.7980 - accuracy: 0.0124\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 197us/sample - loss: 1264.7275 - accuracy: 0.0124\n",
      "Epoch 2/10\n",
      "3384/3384 [==============================] - 0s 49us/sample - loss: 805.4671 - accuracy: 0.0124\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 475.5715 - accuracy: 0.0124\n",
      "Epoch 4/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 259.8399 - accuracy: 0.0124\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 47us/sample - loss: 127.4844 - accuracy: 0.0124\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 47us/sample - loss: 55.0307 - accuracy: 0.0124\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 43us/sample - loss: 16.3274 - accuracy: 0.0124\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 52us/sample - loss: 2.6093 - accuracy: 0.0124\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 43us/sample - loss: 1.3352 - accuracy: 0.0124\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 1.3342 - accuracy: 0.0124\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 232us/sample - loss: 13223.0430 - accuracy: 0.0124\n",
      "Epoch 2/10\n",
      "3384/3384 [==============================] - 0s 43us/sample - loss: 8681.3068 - accuracy: 0.0124\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 36us/sample - loss: 5294.9128 - accuracy: 0.0124\n",
      "Epoch 4/10\n",
      "3384/3384 [==============================] - 0s 36us/sample - loss: 2980.1138 - accuracy: 0.0124\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 1455.2602 - accuracy: 0.0124\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 553.8343 - accuracy: 0.0124\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 37us/sample - loss: 134.3333 - accuracy: 0.0124\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 45us/sample - loss: 21.8553 - accuracy: 0.0124\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 44us/sample - loss: 6.8632 - accuracy: 0.0124\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 41us/sample - loss: 6.7406 - accuracy: 0.0124\n",
      "Shapes of train: (3384,), validation: (70,), test: (53,).\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 170us/sample - loss: 0.7396 - accuracy: 0.0102\n",
      "Epoch 2/10\n",
      "3384/3384 [==============================] - 0s 43us/sample - loss: 0.7374 - accuracy: 0.0102\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 41us/sample - loss: 0.7368 - accuracy: 0.0102\n",
      "Epoch 4/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 0.7367 - accuracy: 0.0102\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 0.7366 - accuracy: 0.0102\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 37us/sample - loss: 0.7366 - accuracy: 0.0102\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 44us/sample - loss: 0.7366 - accuracy: 0.0102\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 41us/sample - loss: 0.7365 - accuracy: 0.0102\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 41us/sample - loss: 0.7365 - accuracy: 0.0102\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 0.7365 - accuracy: 0.0102\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 248us/sample - loss: 13.8137 - accuracy: 0.0102\n",
      "Epoch 2/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 9.3385 - accuracy: 0.0102\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 6.0914 - accuracy: 0.0102\n",
      "Epoch 4/10\n",
      "3384/3384 [==============================] - 0s 36us/sample - loss: 3.8064 - accuracy: 0.0102\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 35us/sample - loss: 2.3034 - accuracy: 0.0102\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 33us/sample - loss: 1.3863 - accuracy: 0.0102\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 0.9340 - accuracy: 0.0102\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 42us/sample - loss: 0.7665 - accuracy: 0.0102\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 0.7465 - accuracy: 0.0102\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 0.7464 - accuracy: 0.0102\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 192us/sample - loss: 128.4926 - accuracy: 0.0102\n",
      "Epoch 2/10\n",
      "3384/3384 [==============================] - 0s 56us/sample - loss: 84.4714 - accuracy: 0.0102\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 51.7839 - accuracy: 0.0102\n",
      "Epoch 4/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 29.3025 - accuracy: 0.0102\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 43us/sample - loss: 14.9942 - accuracy: 0.0102\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 47us/sample - loss: 6.9464 - accuracy: 0.0102\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 2.5377 - accuracy: 0.0102\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 41us/sample - loss: 0.9559 - accuracy: 0.0102\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 41us/sample - loss: 0.7985 - accuracy: 0.0102\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 0.7981 - accuracy: 0.0102\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 202us/sample - loss: 1244.7273 - accuracy: 0.0102\n",
      "Epoch 2/10\n",
      "3384/3384 [==============================] - 0s 53us/sample - loss: 790.4702 - accuracy: 0.0102\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 137us/sample - loss: 465.8093 - accuracy: 0.0102\n",
      "Epoch 4/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 262.9333 - accuracy: 0.0102\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 131.0315 - accuracy: 0.0102\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 56.0758 - accuracy: 0.0102\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 39us/sample - loss: 18.9464 - accuracy: 0.0102\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 46us/sample - loss: 3.2613 - accuracy: 0.0102\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 44us/sample - loss: 1.3281 - accuracy: 0.0102\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 41us/sample - loss: 1.3282 - accuracy: 0.0102\n",
      "Input size for the neural network was: 16\n",
      "Train on 3384 samples\n",
      "Epoch 1/10\n",
      "3384/3384 [==============================] - 1s 188us/sample - loss: 12808.3640 - accuracy: 0.0102\n",
      "Epoch 2/10\n",
      "3384/3384 [==============================] - 0s 46us/sample - loss: 8251.8711 - accuracy: 0.0102\n",
      "Epoch 3/10\n",
      "3384/3384 [==============================] - 0s 46us/sample - loss: 4875.6207 - accuracy: 0.0102\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3384/3384 [==============================] - 0s 42us/sample - loss: 2605.5542 - accuracy: 0.0102\n",
      "Epoch 5/10\n",
      "3384/3384 [==============================] - 0s 42us/sample - loss: 1258.2676 - accuracy: 0.0102\n",
      "Epoch 6/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 463.4276 - accuracy: 0.0102\n",
      "Epoch 7/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 109.5737 - accuracy: 0.0102\n",
      "Epoch 8/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 10.0105 - accuracy: 0.0102\n",
      "Epoch 9/10\n",
      "3384/3384 [==============================] - 0s 38us/sample - loss: 6.8571 - accuracy: 0.0102\n",
      "Epoch 10/10\n",
      "3384/3384 [==============================] - 0s 40us/sample - loss: 6.8491 - accuracy: 0.0102\n",
      "Training with the best lambda: 1 on entire training set...\n",
      "Input size for the neural network was: 16\n",
      "Train on 211 samples\n",
      "Epoch 1/10\n",
      "211/211 [==============================] - 0s 2ms/sample - loss: 149.5711 - accuracy: 0.0107\n",
      "Epoch 2/10\n",
      "211/211 [==============================] - 0s 58us/sample - loss: 145.8541 - accuracy: 0.0107\n",
      "Epoch 3/10\n",
      "211/211 [==============================] - 0s 52us/sample - loss: 142.2008 - accuracy: 0.0107\n",
      "Epoch 4/10\n",
      "211/211 [==============================] - 0s 50us/sample - loss: 138.6158 - accuracy: 0.0107\n",
      "Epoch 5/10\n",
      "211/211 [==============================] - 0s 60us/sample - loss: 135.0689 - accuracy: 0.0107\n",
      "Epoch 6/10\n",
      "211/211 [==============================] - 0s 67us/sample - loss: 131.5768 - accuracy: 0.0107\n",
      "Epoch 7/10\n",
      "211/211 [==============================] - 0s 58us/sample - loss: 128.1509 - accuracy: 0.0107\n",
      "Epoch 8/10\n",
      "211/211 [==============================] - 0s 49us/sample - loss: 124.7829 - accuracy: 0.0107\n",
      "Epoch 9/10\n",
      "211/211 [==============================] - 0s 60us/sample - loss: 121.4597 - accuracy: 0.0107\n",
      "Epoch 10/10\n",
      "211/211 [==============================] - 0s 53us/sample - loss: 118.1808 - accuracy: 0.0107\n"
     ]
    }
   ],
   "source": [
    "# a, b, c = model_builder(\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     feature_names=['first_influence_matrix'],\n",
    "#     estimation_name=estimation_name,\n",
    "#     error_type_str=ERROR_TYPE_STR,\n",
    "#     tune_hyperparameters_by_validation=True,\n",
    "#     with_replication=WITH_REPLICATION,\n",
    "#     lambdas=LAMBDAS,\n",
    "#     model_func=concatinated_deep_neural_network_model_func,\n",
    "#     params={'n_splits': 3, 'n_epochs': 10, 'batch_size': 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3242584311623669"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32196516153850907"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.model_builder.<locals>.<lambda>>,\n",
       "            {0: 1.1056997508973119,\n",
       "             0.1: 0.9879060022903541,\n",
       "             1: 0.9876533514632948,\n",
       "             10: 0.9879012837935006,\n",
       "             100: 0.9880853469198327})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
