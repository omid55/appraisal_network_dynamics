{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# Generating a supervised dataset from the Jeopardy-like logs\n",
    "# ===========================================================\n",
    "\n",
    "## Goals:\n",
    "####   1. Generate different networks from log (sentiment, emotion, and reply duration based)\n",
    "####   2. Generate text embedding data\n",
    "####   3. Map all to influence (appraisal) matrix as the groundtruth to estimate\n",
    "####   4. Use LSTM to take the order (time) also into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last update: 02 Dec 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omid/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import heapq\n",
    "import imp\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "%matplotlib inline\n",
    "\n",
    "# import Softmax_Loss\n",
    "import text_processor\n",
    "import pogs_jeopardy_log_lib\n",
    "import broadcast_network_extraction\n",
    "import utils\n",
    "from mytimer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload():\n",
    "    imp.reload(pogs_jeopardy_log_lib)\n",
    "    imp.reload(text_processor)\n",
    "    imp.reload(utils)\n",
    "    imp.reload(broadcast_network_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_extractor = broadcast_network_extraction.NetworkExtraction()\n",
    "content_fixer = text_processor.FormalEnglishTranslator('../bagofwords/slang.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/home/omid/Datasets/Jeopardy/'\n",
    "time_window = [2, 10]\n",
    "apply_content_fixer = True\n",
    "fix_spelling = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading teams' logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing team 7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/pogs_jeopardy_log_lib.py:392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  event_log_no_message[\"sender_subject_id\"] = pd.to_numeric(event_log_no_message[\"sender_subject_id\"])\n",
      "../src/pogs_jeopardy_log_lib.py:642: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  elNoMessage[\"sender_subject_id\"] = pd.to_numeric(elNoMessage[\"sender_subject_id\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing team 10 ...\n",
      "Processing team 11 ...\n",
      "Processing team 12 ...\n",
      "Processing team 13 ...\n",
      "Processing team 14 ...\n",
      "Processing team 15 ...\n",
      "Processing team 16 ...\n",
      "Processing team 17 ...\n",
      "Processing team 19 ...\n",
      "Processing team 20 ...\n",
      "Processing team 21 ...\n",
      "Processing team 22 ...\n",
      "Processing team 23 ...\n",
      "Processing team 27 ...\n",
      "Processing team 28 ...\n",
      "Processing team 30 ...\n",
      "Processing team 31 ...\n",
      "Processing team 32 ...\n",
      "Processing team 33 ...\n",
      "Processing team 34 ...\n",
      "Processing team 35 ...\n",
      "Processing team 36 ...\n",
      "Processing team 37 ...\n",
      "Processing team 38 ...\n",
      "Processing team 39 ...\n",
      "Processing team 40 ...\n",
      "Processing team 41 ...\n",
      "Processing team 42 ...\n",
      "Processing team 43 ...\n",
      "Processing team 44 ...\n",
      "Processing team 45 ...\n",
      "Processing team 46 ...\n",
      "Processing team 47 ...\n",
      "Processing team 48 ...\n",
      "Processing team 49 ...\n",
      "Processing team 50 ...\n",
      "Team 50 is not found in the logs. There is nothing we can do.\n",
      "Processing team 54 ...\n",
      "Team 54 is not found in the logs. There is nothing we can do.\n",
      "Processing team 61 ...\n",
      "Team 61 is not found in the logs. There is nothing we can do.\n",
      "Processing team 62 ...\n",
      "Team 62 is not found in the logs. There is nothing we can do.\n",
      "Processing team 63 ...\n",
      "Team 63 is not found in the logs. There is nothing we can do.\n",
      "Processing team 69 ...\n",
      "Team 69 is not found in the logs. There is nothing we can do.\n",
      "Processing team 70 ...\n",
      "Processing team 71 ...\n",
      "Processing team 72 ...\n",
      "Processing team 73 ...\n",
      "Processing team 74 ...\n",
      "Processing team 75 ...\n",
      "Processing team 77 ...\n",
      "Processing team 79 ...\n",
      "Processing team 80 ...\n",
      "Team 80 is not found in the logs. There is nothing we can do.\n",
      "Processing team 82 ...\n",
      "Processing team 84 ...\n",
      "Processing team 85 ...\n",
      "Processing team 86 ...\n",
      "Team 86 is not found in the logs. There is nothing we can do.\n",
      "Processing team 87 ...\n",
      "Processing team 88 ...\n",
      "It took 12.35 minutes.\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    teams = pd.read_csv(\n",
    "        directory+\"team.csv\",\n",
    "        sep=',',\n",
    "        quotechar=\"|\",\n",
    "        names=[\"id\",\"sessionId\",\"roundId\", \"taskId\"])\n",
    "    data = {}\n",
    "    for team_id in teams.id:\n",
    "        print(\"Processing team\", team_id, '...')\n",
    "        try:\n",
    "            data[team_id] = pogs_jeopardy_log_lib.TeamLogProcessor(\n",
    "                team_id=team_id, logs_directory_path=directory)\n",
    "        except pogs_jeopardy_log_lib.EventLogsNotLoadedError as e:\n",
    "            print('Team {} is not found in the logs. There is nothing we can do.'.format(team_id))\n",
    "            continue\n",
    "        except Exception as e2:\n",
    "            print('Team {} had some problems. Check.'.format(team_id))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omid/Datasets/Jeopardy/Teams_logs.pk is successfully saved.\n"
     ]
    }
   ],
   "source": [
    "utils.save_it(data, directory+'Teams_logs.pk', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 70, 71, 72, 73, 74, 75, 77, 79, 82, 84, 85, 87, 88])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 :  8 9\n",
      "16 :  3 2\n",
      "27 :  2 1\n",
      "35 :  1 0\n",
      "40 :  7 6\n",
      "70 :  7 8\n"
     ]
    }
   ],
   "source": [
    "# for team_id, team_log in data.items():\n",
    "#     messagesby5 = len(team_log.messages) // 5\n",
    "#     matrices = len(team_log.member_influences)\n",
    "#     if messagesby5 != matrices:\n",
    "#         print(team_id, ': ', messagesby5, matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing the language of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 1.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    if apply_content_fixer:\n",
    "        for team_id, team_log in data.items():\n",
    "            for i in range(len(team_log.messages)):\n",
    "                team_log.messages[i] = content_fixer.translate_messages(\n",
    "                    messages=team_log.messages[i],\n",
    "                    message_column_name='event_content',\n",
    "                    fix_spelling=fix_spelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comibing logs before reporting the appraisal matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing team 7 ...\n",
      "Processing team 10 ...\n",
      "Processing team 11 ...\n",
      "Processing team 12 ...\n",
      "Processing team 13 ...\n",
      "Processing team 14 ...\n",
      "Processing team 15 ...\n",
      "Processing team 16 ...\n",
      "Processing team 17 ...\n",
      "Processing team 19 ...\n",
      "Processing team 20 ...\n",
      "Processing team 21 ...\n",
      "Processing team 22 ...\n",
      "Processing team 23 ...\n",
      "Processing team 27 ...\n",
      "Processing team 28 ...\n",
      "Processing team 30 ...\n",
      "Processing team 31 ...\n",
      "Processing team 32 ...\n",
      "Processing team 33 ...\n",
      "Processing team 34 ...\n",
      "Processing team 35 ...\n",
      "Team 35 does not have enough logs.\n",
      "Processing team 36 ...\n",
      "Processing team 37 ...\n",
      "Processing team 38 ...\n",
      "Processing team 39 ...\n",
      "Processing team 40 ...\n",
      "Processing team 41 ...\n",
      "Processing team 42 ...\n",
      "Processing team 43 ...\n",
      "Processing team 44 ...\n",
      "Processing team 45 ...\n",
      "Processing team 46 ...\n",
      "Processing team 47 ...\n",
      "Processing team 48 ...\n",
      "Processing team 49 ...\n",
      "Processing team 70 ...\n",
      "Processing team 71 ...\n",
      "Processing team 72 ...\n",
      "Team 72 does not have enough logs.\n",
      "Processing team 73 ...\n",
      "Team 73 does not have enough logs.\n",
      "Processing team 74 ...\n",
      "Processing team 75 ...\n",
      "Processing team 77 ...\n",
      "Processing team 79 ...\n",
      "Processing team 82 ...\n",
      "Processing team 84 ...\n",
      "Processing team 85 ...\n",
      "Processing team 87 ...\n",
      "Processing team 88 ...\n"
     ]
    }
   ],
   "source": [
    "combined_logs = {}\n",
    "for team_id, team_log in data.items():\n",
    "    print(\"Processing team\", team_id, '...')\n",
    "    this_team_nets = []\n",
    "    this_team_number_of_networks = min(\n",
    "        len(team_log.messages) // 5,\n",
    "        len(team_log.member_influences))\n",
    "    all_messages_before_appraisal_reports = []\n",
    "    for i in range(this_team_number_of_networks):\n",
    "        all_messages_before_appraisal_reports.append(\n",
    "            pd.concat(\n",
    "                [team_log.messages[i] for i in np.arange(i * 5, i * 5 + 5)]))\n",
    "    if len(all_messages_before_appraisal_reports) > 0:\n",
    "        combined_logs[team_id] = all_messages_before_appraisal_reports\n",
    "    else:\n",
    "        print('Team', team_id, 'does not have enough logs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting differnet networks from the combined logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing team 7 ...\n",
      "It took 0.02 seconds.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "extract_network_from_broadcast() got an unexpected keyword argument 'node_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-308-2aef22d90a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0maggregation_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbroadcast_network_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAggregationType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAVERAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 node_list=data[team_id].members)\n\u001b[0m\u001b[1;32m     14\u001b[0m             sentiment_net = net_extractor.extract_network_from_broadcast(\n\u001b[1;32m     15\u001b[0m                 \u001b[0mcommunication_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_messages_before_appraisal_report\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: extract_network_from_broadcast() got an unexpected keyword argument 'node_list'"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    networks = {}\n",
    "    for team_id, all_messages_before_appraisal_reports in combined_logs.items():\n",
    "        print(\"Processing team\", team_id, '...')\n",
    "        this_team_nets = []\n",
    "        for all_messages_before_appraisal_report in all_messages_before_appraisal_reports:\n",
    "            reply_duration_net = net_extractor.extract_network_from_broadcast(            \n",
    "                communication_data=all_messages_before_appraisal_report,\n",
    "                time_window=time_window,\n",
    "                weight_type=broadcast_network_extraction.WeightType.REPLY_DURATION,\n",
    "                aggregation_type=broadcast_network_extraction.AggregationType.AVERAGE,\n",
    "                gamma=0.15,\n",
    "                node_list=data[team_id].members)\n",
    "            sentiment_net = net_extractor.extract_network_from_broadcast(\n",
    "                communication_data=all_messages_before_appraisal_report,\n",
    "                time_window=time_window,\n",
    "                weight_type=broadcast_network_extraction.WeightType.SENTIMENT,\n",
    "                aggregation_type=broadcast_network_extraction.AggregationType.AVERAGE,  # SUM\n",
    "                node_list=data[team_id].members)\n",
    "            emotion_arousal_net = net_extractor.extract_network_from_broadcast(\n",
    "                communication_data=all_messages_before_appraisal_report,\n",
    "                time_window=time_window,\n",
    "                weight_type=broadcast_network_extraction.WeightType.EMOTION_AROUSAL,\n",
    "                aggregation_type=broadcast_network_extraction.AggregationType.AVERAGE, # SUM\n",
    "                node_list=data[team_id].members)\n",
    "            emotion_dominance_net = net_extractor.extract_network_from_broadcast(\n",
    "                communication_data=all_messages_before_appraisal_report,\n",
    "                time_window=time_window,\n",
    "                weight_type=broadcast_network_extraction.WeightType.EMOTION_DOMINANCE,\n",
    "                aggregation_type=broadcast_network_extraction.AggregationType.AVERAGE, # SUM\n",
    "                node_list=data[team_id].members)\n",
    "            emotion_valence_net = net_extractor.extract_network_from_broadcast(\n",
    "                communication_data=all_messages_before_appraisal_report,\n",
    "                time_window=time_window,\n",
    "                weight_type=broadcast_network_extraction.WeightType.EMOTION_VALENCE,\n",
    "                aggregation_type=broadcast_network_extraction.AggregationType.AVERAGE, # SUM\n",
    "                node_list=data[team_id].members)\n",
    "            if len(reply_duration_net.nodes()) > 0:\n",
    "                this_team_nets.append({\n",
    "                    'sentiment': sentiment_net,\n",
    "                    'reply_duration': reply_duration_net,\n",
    "                    'emotion_arousal': emotion_arousal_net,\n",
    "                    'emotion_dominance': emotion_dominance_net,\n",
    "                    'emotion_valence': emotion_valence_net})\n",
    "        if len(this_team_nets) > 0:\n",
    "            networks[team_id] = this_team_nets\n",
    "        else:\n",
    "            print('Team', team_id, 'did not have enough networks.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omid/Datasets/Jeopardy/Teams_networks.pk is successfully saved.\n"
     ]
    }
   ],
   "source": [
    "utils.save_it(networks, directory+'Teams_networks.pk', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theses teams did not have networks:  {72, 73, 35}\n"
     ]
    }
   ],
   "source": [
    "print('Theses teams did not have networks: ',\n",
    "      set(data.keys()) - set(networks.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting content of all texts that every person sent from combined logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing team 7 ...\n",
      "Processing team 10 ...\n",
      "Processing team 11 ...\n",
      "Processing team 12 ...\n",
      "Processing team 13 ...\n",
      "Processing team 14 ...\n",
      "Processing team 15 ...\n",
      "Processing team 16 ...\n",
      "Processing team 17 ...\n",
      "Processing team 19 ...\n",
      "Processing team 20 ...\n",
      "Processing team 21 ...\n",
      "Processing team 22 ...\n",
      "Processing team 23 ...\n",
      "Processing team 27 ...\n",
      "Processing team 28 ...\n",
      "Processing team 30 ...\n",
      "Processing team 31 ...\n",
      "Processing team 32 ...\n",
      "Processing team 33 ...\n",
      "Processing team 34 ...\n",
      "Processing team 36 ...\n",
      "Processing team 37 ...\n",
      "Processing team 38 ...\n",
      "Processing team 39 ...\n",
      "Processing team 40 ...\n",
      "Processing team 41 ...\n",
      "Processing team 42 ...\n",
      "Processing team 43 ...\n",
      "Processing team 44 ...\n",
      "Processing team 45 ...\n",
      "Processing team 46 ...\n",
      "Processing team 47 ...\n",
      "Processing team 48 ...\n",
      "Processing team 49 ...\n",
      "Processing team 70 ...\n",
      "Processing team 71 ...\n",
      "Processing team 74 ...\n",
      "Processing team 75 ...\n",
      "Processing team 77 ...\n",
      "Processing team 79 ...\n",
      "Processing team 82 ...\n",
      "Processing team 84 ...\n",
      "Processing team 85 ...\n",
      "Processing team 87 ...\n",
      "Processing team 88 ...\n"
     ]
    }
   ],
   "source": [
    "contents = {}\n",
    "for team_id, all_messages_before_appraisal_reports in combined_logs.items():\n",
    "    print(\"Processing team\", team_id, '...')\n",
    "    member_concat_messages = []\n",
    "    for all_messages_before_appraisal_report in all_messages_before_appraisal_reports:\n",
    "        this_time_member_concat_messages = []\n",
    "        for member in sorted(data[team_id].members):\n",
    "            this_time_member_concat_messages.append(' '.join(\n",
    "                all_messages_before_appraisal_report[\n",
    "                all_messages_before_appraisal_report.sender_subject_id == member].event_content))\n",
    "        member_concat_messages.append(this_time_member_concat_messages)\n",
    "    contents[team_id] = member_concat_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omid/Datasets/Jeopardy/Teams_contents.pk is successfully saved.\n"
     ]
    }
   ],
   "source": [
    "utils.save_it(contents, directory+'Teams_contents.pk', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing team 7 ...\n",
      "Processing team 10 ...\n",
      "Processing team 11 ...\n",
      "Processing team 12 ...\n",
      "Processing team 13 ...\n",
      "Processing team 14 ...\n",
      "Processing team 15 ...\n",
      "Processing team 16 ...\n",
      "Processing team 17 ...\n",
      "Processing team 19 ...\n",
      "Processing team 20 ...\n",
      "Processing team 21 ...\n",
      "Processing team 22 ...\n",
      "Processing team 23 ...\n",
      "Processing team 27 ...\n",
      "Processing team 28 ...\n",
      "Processing team 30 ...\n",
      "Processing team 31 ...\n",
      "Processing team 32 ...\n",
      "Processing team 33 ...\n",
      "Processing team 34 ...\n",
      "Processing team 35 ...\n",
      "Processing team 36 ...\n",
      "Processing team 37 ...\n",
      "Processing team 38 ...\n",
      "Processing team 39 ...\n",
      "Processing team 40 ...\n",
      "Processing team 41 ...\n",
      "Processing team 42 ...\n",
      "Processing team 43 ...\n",
      "Processing team 44 ...\n",
      "Processing team 45 ...\n",
      "Processing team 46 ...\n",
      "Processing team 47 ...\n",
      "Processing team 48 ...\n",
      "Processing team 49 ...\n",
      "Processing team 70 ...\n",
      "Processing team 71 ...\n",
      "Processing team 72 ...\n",
      "Processing team 73 ...\n",
      "Processing team 74 ...\n",
      "Processing team 75 ...\n",
      "Processing team 77 ...\n",
      "Processing team 79 ...\n",
      "Processing team 82 ...\n",
      "Processing team 84 ...\n",
      "Processing team 85 ...\n",
      "Processing team 87 ...\n",
      "Processing team 88 ...\n"
     ]
    }
   ],
   "source": [
    "supervised_data = []\n",
    "for team_id, team_log in data.items():\n",
    "    print(\"Processing team\", team_id, '...')\n",
    "    if team_id in networks:\n",
    "        for index in range(len(networks[team_id])):\n",
    "            network = networks[team_id][index]\n",
    "            influence_matrix = np.matrix(team_log.member_influences[index])\n",
    "            supervised_data.append({\n",
    "                'influence_matrix': utils.shuffle_matrix_in_given_order(\n",
    "                    matrix=influence_matrix,\n",
    "                    order=np.argsort(team_log.members)),\n",
    "                'reply_duration': nx.adj_matrix(network['reply_duration']).todense(),\n",
    "                'sentiment': nx.adj_matrix(network['sentiment']).todense(),\n",
    "                'emotion_arousal': nx.adj_matrix(network['emotion_arousal']).todense(),\n",
    "                'emotion_dominance': nx.adj_matrix(network['emotion_dominance']).todense(),\n",
    "                'emotion_valence': nx.adj_matrix(network['emotion_valence']).todense()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(supervised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for item in supervised_data:\n",
    "    if item['reply_duration'].shape[0] != 4:\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19076923076923077"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt / 325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
